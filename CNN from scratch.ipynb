{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the data using PyTorch\n",
    "    1. extract-get the data from Fashion MNIST IMAGE DATA \n",
    "    2. Transform put the data in a tensor \n",
    "    3. Load- put the data into an object to make it accessible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this piece of code is unnecessary if we're using a standard dataset already available in torchvision.datasets \n",
    "# but its a great example to show whats happening under the hood in pytorch\n",
    "# to create a dataset in pytorch we need to extend the torch.utils.data.Dataset class\n",
    "# and implement the following methods:\n",
    "\n",
    "class OHLC(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "    def __getitem__(self,index):\n",
    "        r = self.data.iloc[index]\n",
    "        label = torch.tensor(r.is_up_day, dtype=torch.long)\n",
    "        sample = self.normalize(torch.tensor([r.open, r.high, r.low, r.close]))\n",
    "        return sample,label\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "# we can create a dataset object by passing the csv file as an argument to the constructor\n",
    "# and then we can use the dataset object to create a dataloader object by passing it to the DataLoader class\n",
    "# and then we can use the dataloader object to iterate over the dataset in batches.\n",
    "# we can also use the dataloader object to get the number of samples in the dataset.\n",
    "# we can also use the dataloader object to get the number of batches in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're going to ETL the Fashion MNIST dataset\n",
    "\n",
    "train_set = datasets.FashionMNIST(\n",
    "    root=\"./data/FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()])\n",
    ")\n",
    "train_loader = DataLoader(train_set,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 0, 0,  ..., 3, 0, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in our case the dataset is balanced as the bins have equal number of samples\n",
    "# but if we encounter a case of unbalanced dataset then we can use oversampling and undersampling techniques and stratified sampling\n",
    "# we can also use a combination of these techniques to get a balanced dataset \n",
    "# it might be a burden on memory but it yields better resutls \n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "len(train_set)\n",
    "print(train_set.targets) # targets are the labels of the train data\n",
    "train_set.targets.bincount()\n",
    "# we can see that the bins are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regarding the paper \n",
    "**\" A systematic study of the class imbalance problem in convolutional neural networks\"** by\n",
    "https://arxiv.org/abs/1710.05381\n",
    "\n",
    "i quote from the paper : \"Regarding performance of different methods for addressing imbalance, in almost all of the situations oversampling emerged as the best method.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Accessing data in the training set\n",
    "# we use the python built in function iter(),which returns an object representing a stream of data.\n",
    "# The function next() takes this object and returns the next value from the stream.\n",
    "# We can use this to iterate over the data\n",
    "# we get 2 because the data is a tuple of 2 elements, a pair of image and its label\n",
    "\n",
    "sample = next(iter(train_set))\n",
    "print(len(sample))\n",
    "\n",
    "# we can destruct it or sequence unpacking \n",
    "image = sample[0]\n",
    "label = sample[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "print('label:',label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch= next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "grid= torchvision.utils.make_grid(images,nrow=10)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(np.transpose(grid,(1,2,0)))\n",
    "plt.title(\"Labels: \"+ str(labels))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module): \n",
    "    def __init__(self,channels=1):\n",
    "        super().__init__()\n",
    "        # PyTorch's nn.Module class keeps tracks of the wieghts tensors and biases inside each layer, so we pass them as arguments to the constructor\n",
    "        # sets in_channels is for the color channels, if we're working with non linear layers out_channels is the number of filters; one filter gives one output channel/ features , if we're working with linear layers we out_feature sets the size of the output tensor, kernel_size is the size of the kernel/ convolutional filter\n",
    "        # one common thing when building CNN layers is to increase the number of out_channels as we go through the layers, and decrease the number of out_features as we go through linear layers\n",
    " \n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) # 2 convolutional layers \n",
    "        # Second conv with padding=2 to maintain dimensions after pooling\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) # we add a stride that is now 1 by default, to tell the filter how many units to move up and down \n",
    "        # 3 fully connected layers/ linear layers/ dense layers\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # when we switch from convolutional layers to linear layers we need to flatten the output of the convolutional layers this is why we have 12*4*4 , the 12 is the number of output channels from the second convolutional layer, 4*4 means output of the last CNN is 4x4 image/filter:outputSizeOfCov = [(inputSize + 2*pad - filterSize)/stride] + 1\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)# in_features depend on the previous layer's out_features\n",
    "        self.out = nn.Linear(in_features=60, out_features=10) # 10 out_features here are the predicted classes, in this case 10 classes for the Fashion MNIST dataset\n",
    "# Note: in linear layers by default the bias is set to True \n",
    "# All filters are represented using a single tensor.\n",
    "# Filters have depth that accounts for the input channels.\n",
    "# conv weight tensors are of rank 4 (numbers of filters, in_channels, height, width)\n",
    "# linear weight tensors are of rank 2 (out_features,in_features) (H,W) \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t\n",
    "    \n",
    "    #def __repr__(self): # just an example pn some Python OOP that u may help u when debugging\n",
    "    #    return \"this overrides the official string representation of the object\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(2.3167, grad_fn=<NllLossBackward0>)\n",
      "no. correct: tensor(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a network\n",
    "network = Network() \n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return (preds.argmax(dim=1) == labels).sum()\n",
    "\n",
    "F.cross_entropy(torch.tensor([[3, 6, 2], [1, 1, 2]]).float(), torch.tensor([2, 1]))\n",
    "\n",
    "softmax = lambda a: np.exp(a) / np.exp(a).sum()\n",
    "\n",
    "(-np.log(softmax(np.array([3,6,2])))[2] + -np.log(softmax(np.array([1,1,2])))[1])/2 # averages\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) # optimizer has access to network parameters\n",
    "\n",
    "network.conv1.weight.grad is None\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels) \n",
    "\n",
    "print('loss: ', loss) \n",
    "print('no. correct:', get_num_correct(preds, labels)) # out of 100\n",
    "\n",
    "loss.backward() # backprop, looks at definition of loss and crawls backward into the network\n",
    "network.conv1.weight.grad.shape # gradients updated after one pass of backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after backprop:  tensor(2.2388, grad_fn=<NllLossBackward0>) no. correct: tensor(2)\n"
     ]
    }
   ],
   "source": [
    "optimizer.step() # based on our new loss gradient values, we update weights accdg to Adam to minimize loss.\n",
    "\n",
    "preds = network(images) # run new predictions\n",
    "loss = F.cross_entropy(preds, labels) \n",
    "print('loss after backprop: ', loss, 'no. correct:', get_num_correct(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0:\n",
      "2.350571393966675\n",
      "tensor(0)\n",
      "\n",
      "Step 1:\n",
      "2.2857930660247803\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "#Training one batch\n",
    "\n",
    "# compile the neural net\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# loss\n",
    "loss = F.cross_entropy(network(images), labels)\n",
    "print(\"Step 0:\")\n",
    "print(loss.item())\n",
    "print(get_num_correct(network(images), labels))\n",
    "\n",
    "# backprop\n",
    "loss.backward()  # update gradients\n",
    "optimizer.step() # update weights using gradients to minimize loss\n",
    "\n",
    "# recalculating loss based on new weights\n",
    "loss = F.cross_entropy(network(images), labels)\n",
    "print(\"\\nStep 1:\")\n",
    "print(loss.item())\n",
    "print(get_num_correct(network(images), labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: tensor(45379) loss: 380.0166459083557\n"
     ]
    }
   ],
   "source": [
    "#Training a single epoch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100) \n",
    "\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "for batch in train_loader:\n",
    "    images, labels = batch \n",
    "\n",
    "    preds = network(images) \n",
    "    loss = F.cross_entropy(preds, labels) \n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()  # calculate gradients\n",
    "    optimizer.step() # update weights using gradients using adam\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "print(\n",
    "    \"epoch:\", 0, \n",
    "    \"total_correct:\", total_correct, \n",
    "    \"loss:\", total_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1998)\n"
     ]
    }
   ],
   "source": [
    "if network.conv1.weight.grad is not None:\n",
    "    print(network.conv1.weight.grad.sum())\n",
    "else:\n",
    "    print(\"No gradients computed yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: tensor(47719) loss: 324.959082454443\n",
      "epoch 1 total_correct: tensor(51605) loss: 224.59134384989738\n",
      "epoch 2 total_correct: tensor(52353) loss: 205.31755885481834\n",
      "epoch 3 total_correct: tensor(52815) loss: 195.77987752854824\n",
      "epoch 4 total_correct: tensor(52925) loss: 189.5943547040224\n",
      "epoch 5 total_correct: tensor(53135) loss: 184.88502830266953\n",
      "epoch 6 total_correct: tensor(53212) loss: 183.10018488764763\n",
      "epoch 7 total_correct: tensor(53416) loss: 177.53898786008358\n",
      "epoch 8 total_correct: tensor(53282) loss: 179.6652434170246\n",
      "epoch 9 total_correct: tensor(53448) loss: 177.2940489128232\n"
     ]
    }
   ],
   "source": [
    "#Training with multiple epochs\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):    \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader:    \n",
    "        images, labels = batch \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds, labels) # check that loss tensor has a gradient attribute\n",
    "                                              # so that line 17 makes sense\n",
    "        optimizer.zero_grad() # set all gradients to zero\n",
    "        loss.backward() # calculate gradient\n",
    "        optimizer.step() # update Weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train accuracy is 53448/60000 (0.891)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TensorBoard: TensorFlow's Visualization Toolkit**\n",
    "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
    "\n",
    "    1. Tracking and visualizing metrics such as loss and accuracy\n",
    "    2. Visualizing the model graph (ops and layers)\n",
    "    3. Viewing histograms of weights, biases, or other tensors as they change over time\n",
    "    4. Projecting embeddings to a lower dimensional space\n",
    "    5. Displaying images, text, and audio data\n",
    "    6. Visualizing the execution of a TensorFlow graph\n",
    "    7. Visualizing a TensorFlow program's execution\n",
    "    8. Profiling TensorFlow programs\n",
    "    And much more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorboard\\default.py:30: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m198\u001b[0m, in \u001b[35m_run_module_as_main\u001b[0m\n",
      "  File \u001b[35m\"<frozen runpy>\"\u001b[0m, line \u001b[35m88\u001b[0m, in \u001b[35m_run_code\u001b[0m\n",
      "  File \u001b[35m\"c:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\Scripts\\tensorboard.exe\\__main__.py\"\u001b[0m, line \u001b[35m4\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from tensorboard.main import run_main\n",
      "  File \u001b[35m\"C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorboard\\main.py\"\u001b[0m, line \u001b[35m27\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from tensorboard import default\n",
      "  File \u001b[35m\"C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorboard\\default.py\"\u001b[0m, line \u001b[35m40\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from tensorboard.plugins.image import images_plugin\n",
      "  File \u001b[35m\"C:\\Users\\dell\\AppData\\Roaming\\Python\\Python313\\site-packages\\tensorboard\\plugins\\image\\images_plugin.py\"\u001b[0m, line \u001b[35m18\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    import imghdr\n",
      "\u001b[1;35mModuleNotFoundError\u001b[0m: \u001b[35mNo module named 'imghdr'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "!tensorboard --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tb = SummaryWriter()\n",
    "network = Network()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "tb.add_image('images', grid)\n",
    "tb.add_graph(network, images)\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train_acc tensor(0.6970) loss: 477.0766302347183\n",
      "epoch 1 train_acc tensor(0.8063) loss: 307.8935236334801\n",
      "epoch 2 train_acc tensor(0.8407) loss: 260.4904620051384\n",
      "epoch 3 train_acc tensor(0.8589) loss: 231.5982717424631\n",
      "epoch 4 train_acc tensor(0.8694) loss: 213.44451256096363\n",
      "epoch 5 train_acc tensor(0.8774) loss: 199.79611791670322\n",
      "epoch 6 train_acc tensor(0.8840) loss: 189.5195411592722\n",
      "epoch 7 train_acc tensor(0.8889) loss: 181.08475697040558\n",
      "epoch 8 train_acc tensor(0.8936) loss: 173.766216263175\n",
      "epoch 9 train_acc tensor(0.8964) loss: 168.2620684877038\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile network\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize tensorboard\n",
    "tb = SummaryWriter() # from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Training\n",
    "for epoch in range(10): \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch \n",
    "        preds = network(images)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, labels) # loss function\n",
    "        optimizer.zero_grad()                 # set all gradients to zero\n",
    "        \n",
    "        loss.backward()         # calculate gradients, training points are supply constants\n",
    "        optimizer.step()        # update weights to minimize loss (accdg to adam)\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "    \n",
    "    tb.add_scalar('Loss', total_loss, epoch)\n",
    "    tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "    tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "    \n",
    "    tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "    tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "    tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "    print(\"epoch\", epoch, \"train_acc\", total_correct / 60000, \"loss:\", total_loss)\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(lr, batch_size, shuffle, num_epochs=5):\n",
    "    # data loader\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # compile network\n",
    "    network = Network()\n",
    "    optimizer = optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "    # Initialize tensorboard\n",
    "    tb = SummaryWriter(comment=f' lr={lr} batch_size={batch_size} shuffle={shuffle}') # this is appended\n",
    "    \n",
    "    # Training\n",
    "    print('\\nlr=', lr, 'batch_size=', batch_size, 'shuffle=', shuffle)\n",
    "    for epoch in range(num_epochs): \n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch \n",
    "            preds = network(images)\n",
    "\n",
    "            loss = F.cross_entropy(preds, labels) \n",
    "            \n",
    "            optimizer.zero_grad()                 \n",
    "            loss.backward()         \n",
    "            optimizer.step()        \n",
    "\n",
    "            total_loss += loss.item()*batch_size             # get absolute loss \n",
    "            total_correct += get_num_correct(preds, labels)\n",
    "\n",
    "        tb.add_scalar('Loss', total_loss, epoch)\n",
    "        tb.add_scalar('Number Correct', total_correct, epoch)\n",
    "        tb.add_scalar('Accuracy', total_correct / len(train_set), epoch)\n",
    "\n",
    "        tb.add_histogram('conv1.bias', network.conv1.bias, epoch)\n",
    "        tb.add_histogram('conv1.weight', network.conv1.weight, epoch)\n",
    "        tb.add_histogram('conv1.weight.grad', network.conv1.weight.grad, epoch)\n",
    "\n",
    "        print(\"epoch\", epoch, \"\\t train_acc\", total_correct / len(train_set), \"\\t loss:\", total_loss)\n",
    "\n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0-dev20250708'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow\n",
    "\n",
    "tensorflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lr= 0.1 batch_size= 10 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.1026) \t loss: 140034.82088804245\n",
      "epoch 1 \t train_acc tensor(0.0992) \t loss: 139563.37186574936\n",
      "epoch 2 \t train_acc tensor(0.0997) \t loss: 139587.29279994965\n",
      "epoch 3 \t train_acc tensor(0.1014) \t loss: 139564.21141147614\n",
      "epoch 4 \t train_acc tensor(0.0997) \t loss: 139527.795317173\n",
      "epoch 5 \t train_acc tensor(0.1015) \t loss: 139481.81215763092\n",
      "epoch 6 \t train_acc tensor(0.0994) \t loss: 139517.7787733078\n",
      "epoch 7 \t train_acc tensor(0.0973) \t loss: 139589.9741768837\n",
      "epoch 8 \t train_acc tensor(0.1000) \t loss: 139563.27685117722\n",
      "epoch 9 \t train_acc tensor(0.1000) \t loss: 139540.8569598198\n",
      "\n",
      "lr= 0.1 batch_size= 10 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.1021) \t loss: 139658.3265209198\n",
      "epoch 1 \t train_acc tensor(0.1021) \t loss: 139448.97980213165\n",
      "epoch 2 \t train_acc tensor(0.1021) \t loss: 139448.97879838943\n",
      "epoch 3 \t train_acc tensor(0.1021) \t loss: 139448.9786529541\n",
      "epoch 4 \t train_acc tensor(0.1021) \t loss: 139448.978703022\n",
      "epoch 5 \t train_acc tensor(0.1021) \t loss: 139448.97867679596\n",
      "epoch 6 \t train_acc tensor(0.1021) \t loss: 139448.97871017456\n",
      "epoch 7 \t train_acc tensor(0.1021) \t loss: 139448.97867918015\n",
      "epoch 8 \t train_acc tensor(0.1021) \t loss: 139448.97874355316\n",
      "epoch 9 \t train_acc tensor(0.1021) \t loss: 139448.97877693176\n",
      "\n",
      "lr= 0.1 batch_size= 100 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.1000) \t loss: 150735.3367805481\n",
      "epoch 1 \t train_acc tensor(0.0980) \t loss: 138597.31061458588\n",
      "epoch 2 \t train_acc tensor(0.0992) \t loss: 138573.3451604843\n",
      "epoch 3 \t train_acc tensor(0.1005) \t loss: 138549.4908809662\n",
      "epoch 4 \t train_acc tensor(0.1007) \t loss: 138572.1785068512\n",
      "epoch 5 \t train_acc tensor(0.1010) \t loss: 138603.18875312805\n",
      "epoch 6 \t train_acc tensor(0.1019) \t loss: 138539.73512649536\n",
      "epoch 7 \t train_acc tensor(0.0987) \t loss: 138570.36147117615\n",
      "epoch 8 \t train_acc tensor(0.0981) \t loss: 138617.81334877014\n",
      "epoch 9 \t train_acc tensor(0.1000) \t loss: 138595.92294692993\n",
      "\n",
      "lr= 0.1 batch_size= 100 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.0994) \t loss: 142955.20207881927\n",
      "epoch 1 \t train_acc tensor(0.0992) \t loss: 138611.19737625122\n",
      "epoch 2 \t train_acc tensor(0.0997) \t loss: 138620.02229690552\n",
      "epoch 3 \t train_acc tensor(0.0998) \t loss: 138623.4926700592\n",
      "epoch 4 \t train_acc tensor(0.0996) \t loss: 138625.09315013885\n",
      "epoch 5 \t train_acc tensor(0.0996) \t loss: 138625.87971687317\n",
      "epoch 6 \t train_acc tensor(0.0996) \t loss: 138626.27956867218\n",
      "epoch 7 \t train_acc tensor(0.0996) \t loss: 138626.48496627808\n",
      "epoch 8 \t train_acc tensor(0.0996) \t loss: 138626.59213542938\n",
      "epoch 9 \t train_acc tensor(0.0996) \t loss: 138626.6473054886\n",
      "\n",
      "lr= 0.1 batch_size= 1000 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.0996) \t loss: 200411.68880462646\n",
      "epoch 1 \t train_acc tensor(0.0996) \t loss: 138301.55324935913\n",
      "epoch 2 \t train_acc tensor(0.1013) \t loss: 138370.59426307678\n",
      "epoch 3 \t train_acc tensor(0.1032) \t loss: 138268.57018470764\n",
      "epoch 4 \t train_acc tensor(0.1011) \t loss: 138276.98469161987\n",
      "epoch 5 \t train_acc tensor(0.0991) \t loss: 138263.7221813202\n",
      "epoch 6 \t train_acc tensor(0.1007) \t loss: 138219.07114982605\n",
      "epoch 7 \t train_acc tensor(0.1015) \t loss: 138228.9125919342\n",
      "epoch 8 \t train_acc tensor(0.0994) \t loss: 138251.6450881958\n",
      "epoch 9 \t train_acc tensor(0.0993) \t loss: 138238.09957504272\n",
      "\n",
      "lr= 0.1 batch_size= 1000 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.0999) \t loss: 406487.92839050293\n",
      "epoch 1 \t train_acc tensor(0.0993) \t loss: 138318.83883476257\n",
      "epoch 2 \t train_acc tensor(0.0984) \t loss: 138288.334608078\n",
      "epoch 3 \t train_acc tensor(0.0993) \t loss: 138272.0501422882\n",
      "epoch 4 \t train_acc tensor(0.0992) \t loss: 138289.88981246948\n",
      "epoch 5 \t train_acc tensor(0.0998) \t loss: 138256.39939308167\n",
      "epoch 6 \t train_acc tensor(0.0994) \t loss: 138251.69563293457\n",
      "epoch 7 \t train_acc tensor(0.0993) \t loss: 138247.08223342896\n",
      "epoch 8 \t train_acc tensor(0.0993) \t loss: 138248.60763549805\n",
      "epoch 9 \t train_acc tensor(0.0992) \t loss: 138251.46961212158\n",
      "\n",
      "lr= 0.01 batch_size= 10 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.7497) \t loss: 39762.435608627275\n",
      "epoch 1 \t train_acc tensor(0.8047) \t loss: 32474.47282521054\n",
      "epoch 2 \t train_acc tensor(0.8102) \t loss: 31936.122927786782\n",
      "epoch 3 \t train_acc tensor(0.8147) \t loss: 31582.056362624862\n",
      "epoch 4 \t train_acc tensor(0.8160) \t loss: 31525.130038124043\n",
      "epoch 5 \t train_acc tensor(0.8158) \t loss: 31810.211445093155\n",
      "epoch 6 \t train_acc tensor(0.8132) \t loss: 32312.281589382328\n",
      "epoch 7 \t train_acc tensor(0.8139) \t loss: 31941.245006220415\n",
      "epoch 8 \t train_acc tensor(0.8164) \t loss: 31553.050964154536\n",
      "epoch 9 \t train_acc tensor(0.8157) \t loss: 32050.781660359353\n",
      "\n",
      "lr= 0.01 batch_size= 10 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.7551) \t loss: 38511.113920756616\n",
      "epoch 1 \t train_acc tensor(0.7988) \t loss: 32342.784350338625\n",
      "epoch 2 \t train_acc tensor(0.8025) \t loss: 32027.726277667098\n",
      "epoch 3 \t train_acc tensor(0.8054) \t loss: 31771.049491328886\n",
      "epoch 4 \t train_acc tensor(0.8074) \t loss: 31541.02868386195\n",
      "epoch 5 \t train_acc tensor(0.8065) \t loss: 31986.42094234645\n",
      "epoch 6 \t train_acc tensor(0.8094) \t loss: 31403.3505605784\n",
      "epoch 7 \t train_acc tensor(0.8111) \t loss: 31545.866224139463\n",
      "epoch 8 \t train_acc tensor(0.8064) \t loss: 32255.988349895924\n",
      "epoch 9 \t train_acc tensor(0.8057) \t loss: 32561.993888840807\n",
      "\n",
      "lr= 0.01 batch_size= 100 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.7419) \t loss: 39724.42739009857\n",
      "epoch 1 \t train_acc tensor(0.8423) \t loss: 25638.593864440918\n",
      "epoch 2 \t train_acc tensor(0.8558) \t loss: 23353.64105850458\n",
      "epoch 3 \t train_acc tensor(0.8614) \t loss: 22368.627579510212\n",
      "epoch 4 \t train_acc tensor(0.8657) \t loss: 21789.322094619274\n",
      "epoch 5 \t train_acc tensor(0.8684) \t loss: 21153.03748100996\n",
      "epoch 6 \t train_acc tensor(0.8704) \t loss: 20933.217646181583\n",
      "epoch 7 \t train_acc tensor(0.8723) \t loss: 20696.047462522984\n",
      "epoch 8 \t train_acc tensor(0.8717) \t loss: 20483.241879940033\n",
      "epoch 9 \t train_acc tensor(0.8754) \t loss: 20119.937007129192\n",
      "\n",
      "lr= 0.01 batch_size= 100 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.7863) \t loss: 33998.285917937756\n",
      "epoch 1 \t train_acc tensor(0.8616) \t loss: 22603.562112152576\n",
      "epoch 2 \t train_acc tensor(0.8748) \t loss: 20387.15229034424\n",
      "epoch 3 \t train_acc tensor(0.8802) \t loss: 19424.070413410664\n",
      "epoch 4 \t train_acc tensor(0.8844) \t loss: 18845.949365198612\n",
      "epoch 5 \t train_acc tensor(0.8853) \t loss: 18771.68480157852\n",
      "epoch 6 \t train_acc tensor(0.8869) \t loss: 18321.576017141342\n",
      "epoch 7 \t train_acc tensor(0.8894) \t loss: 17975.19514411688\n",
      "epoch 8 \t train_acc tensor(0.8901) \t loss: 17772.676495462656\n",
      "epoch 9 \t train_acc tensor(0.8917) \t loss: 17613.481990993023\n",
      "\n",
      "lr= 0.01 batch_size= 1000 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.6291) \t loss: 58688.59529495239\n",
      "epoch 1 \t train_acc tensor(0.7842) \t loss: 33046.63389921188\n",
      "epoch 2 \t train_acc tensor(0.8287) \t loss: 27366.194158792496\n",
      "epoch 3 \t train_acc tensor(0.8470) \t loss: 24986.468493938446\n",
      "epoch 4 \t train_acc tensor(0.8618) \t loss: 22528.974920511246\n",
      "epoch 5 \t train_acc tensor(0.8693) \t loss: 21300.736784934998\n",
      "epoch 6 \t train_acc tensor(0.8774) \t loss: 19959.605932235718\n",
      "epoch 7 \t train_acc tensor(0.8817) \t loss: 19076.95034146309\n",
      "epoch 8 \t train_acc tensor(0.8867) \t loss: 18280.324295163155\n",
      "epoch 9 \t train_acc tensor(0.8898) \t loss: 17789.39911723137\n",
      "\n",
      "lr= 0.01 batch_size= 1000 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.6043) \t loss: 61680.209159851074\n",
      "epoch 1 \t train_acc tensor(0.7754) \t loss: 34114.605873823166\n",
      "epoch 2 \t train_acc tensor(0.8154) \t loss: 28942.77137517929\n",
      "epoch 3 \t train_acc tensor(0.8398) \t loss: 25736.837029457092\n",
      "epoch 4 \t train_acc tensor(0.8539) \t loss: 23867.79198050499\n",
      "epoch 5 \t train_acc tensor(0.8669) \t loss: 21655.65636754036\n",
      "epoch 6 \t train_acc tensor(0.8746) \t loss: 20354.241013526917\n",
      "epoch 7 \t train_acc tensor(0.8804) \t loss: 19297.161012887955\n",
      "epoch 8 \t train_acc tensor(0.8863) \t loss: 18461.192101240158\n",
      "epoch 9 \t train_acc tensor(0.8903) \t loss: 17829.514414072037\n",
      "\n",
      "lr= 0.001 batch_size= 10 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.7761) \t loss: 35521.04614244774\n",
      "epoch 1 \t train_acc tensor(0.8540) \t loss: 23874.1191985365\n",
      "epoch 2 \t train_acc tensor(0.8726) \t loss: 20921.583121991716\n",
      "epoch 3 \t train_acc tensor(0.8801) \t loss: 19326.674278725404\n",
      "epoch 4 \t train_acc tensor(0.8866) \t loss: 18145.85155347275\n",
      "epoch 5 \t train_acc tensor(0.8921) \t loss: 17431.735385036736\n",
      "epoch 6 \t train_acc tensor(0.8974) \t loss: 16655.684291951475\n",
      "epoch 7 \t train_acc tensor(0.9001) \t loss: 16030.586649257748\n",
      "epoch 8 \t train_acc tensor(0.9043) \t loss: 15488.225294648146\n",
      "epoch 9 \t train_acc tensor(0.9071) \t loss: 14965.323514357151\n",
      "\n",
      "lr= 0.001 batch_size= 10 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.7758) \t loss: 35357.65467272606\n",
      "epoch 1 \t train_acc tensor(0.8581) \t loss: 23140.367087321356\n",
      "epoch 2 \t train_acc tensor(0.8768) \t loss: 19891.644155143294\n",
      "epoch 3 \t train_acc tensor(0.8888) \t loss: 18087.53952150233\n",
      "epoch 4 \t train_acc tensor(0.8937) \t loss: 16969.90565911634\n",
      "epoch 5 \t train_acc tensor(0.8985) \t loss: 16152.513407462975\n",
      "epoch 6 \t train_acc tensor(0.9030) \t loss: 15443.081883220584\n",
      "epoch 7 \t train_acc tensor(0.9060) \t loss: 14852.632883311162\n",
      "epoch 8 \t train_acc tensor(0.9097) \t loss: 14323.601335517305\n",
      "epoch 9 \t train_acc tensor(0.9123) \t loss: 13810.334288645536\n",
      "\n",
      "lr= 0.001 batch_size= 100 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.7119) \t loss: 46454.638946056366\n",
      "epoch 1 \t train_acc tensor(0.8194) \t loss: 29644.58113014698\n",
      "epoch 2 \t train_acc tensor(0.8475) \t loss: 25263.139525055885\n",
      "epoch 3 \t train_acc tensor(0.8630) \t loss: 22729.412055015564\n",
      "epoch 4 \t train_acc tensor(0.8726) \t loss: 21161.527958512306\n",
      "epoch 5 \t train_acc tensor(0.8806) \t loss: 19837.230575084686\n",
      "epoch 6 \t train_acc tensor(0.8842) \t loss: 19043.965327739716\n",
      "epoch 7 \t train_acc tensor(0.8898) \t loss: 18191.017147898674\n",
      "epoch 8 \t train_acc tensor(0.8927) \t loss: 17658.60045477748\n",
      "epoch 9 \t train_acc tensor(0.8953) \t loss: 17105.407103151083\n",
      "\n",
      "lr= 0.001 batch_size= 100 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.7048) \t loss: 46156.66282474995\n",
      "epoch 1 \t train_acc tensor(0.8134) \t loss: 29608.480352163315\n",
      "epoch 2 \t train_acc tensor(0.8466) \t loss: 25321.121844649315\n",
      "epoch 3 \t train_acc tensor(0.8601) \t loss: 22974.669182300568\n",
      "epoch 4 \t train_acc tensor(0.8684) \t loss: 21379.941318929195\n",
      "epoch 5 \t train_acc tensor(0.8748) \t loss: 20222.015726566315\n",
      "epoch 6 \t train_acc tensor(0.8813) \t loss: 19265.877304971218\n",
      "epoch 7 \t train_acc tensor(0.8856) \t loss: 18514.13507759571\n",
      "epoch 8 \t train_acc tensor(0.8895) \t loss: 17850.23052841425\n",
      "epoch 9 \t train_acc tensor(0.8928) \t loss: 17277.753686904907\n",
      "\n",
      "lr= 0.001 batch_size= 1000 shuffle= True\n",
      "epoch 0 \t train_acc tensor(0.4744) \t loss: 94079.2156457901\n",
      "epoch 1 \t train_acc tensor(0.7132) \t loss: 46153.071105480194\n",
      "epoch 2 \t train_acc tensor(0.7452) \t loss: 40254.724740982056\n",
      "epoch 3 \t train_acc tensor(0.7704) \t loss: 36642.29917526245\n",
      "epoch 4 \t train_acc tensor(0.7879) \t loss: 34055.71502447128\n",
      "epoch 5 \t train_acc tensor(0.7994) \t loss: 32278.098225593567\n",
      "epoch 6 \t train_acc tensor(0.8103) \t loss: 30642.128348350525\n",
      "epoch 7 \t train_acc tensor(0.8219) \t loss: 29189.040303230286\n",
      "epoch 8 \t train_acc tensor(0.8267) \t loss: 28272.653102874756\n",
      "epoch 9 \t train_acc tensor(0.8362) \t loss: 26889.34016227722\n",
      "\n",
      "lr= 0.001 batch_size= 1000 shuffle= False\n",
      "epoch 0 \t train_acc tensor(0.5144) \t loss: 88920.76599597931\n",
      "epoch 1 \t train_acc tensor(0.7169) \t loss: 44271.225333213806\n",
      "epoch 2 \t train_acc tensor(0.7479) \t loss: 38997.46656417847\n",
      "epoch 3 \t train_acc tensor(0.7675) \t loss: 35988.40355873108\n",
      "epoch 4 \t train_acc tensor(0.7808) \t loss: 33908.0531001091\n",
      "epoch 5 \t train_acc tensor(0.7924) \t loss: 32327.356338500977\n",
      "epoch 6 \t train_acc tensor(0.8048) \t loss: 30841.021686792374\n",
      "epoch 7 \t train_acc tensor(0.8147) \t loss: 29660.408228635788\n",
      "epoch 8 \t train_acc tensor(0.8226) \t loss: 28608.62699151039\n",
      "epoch 9 \t train_acc tensor(0.8303) \t loss: 27658.003270626068\n"
     ]
    }
   ],
   "source": [
    "lr_list = [0.1, 0.01, 0.001]\n",
    "batch_size_list = [10, 100, 1000]\n",
    "shuffle_list = [True, False]\n",
    "\n",
    "# hyperparameter grid search\n",
    "for param in product(lr_list, batch_size_list, shuffle_list):\n",
    "    train(*param, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Remarks:**\n",
    "\n",
    "For a large learning rate 0.1, the model does not improve beyond 10% accuracy.\n",
    "We can imitate the sklearn API and define .fit(self, dataset, lr, batch_size, shuffle, num_epochs) inside an instance of the network.\n",
    "\n",
    "### **Summary: End-to-end use.**\n",
    "The SummaryWriter object writes to the runs folder information regarding what are written to it during training. TensorBoard then accesses these log files. The interface can be viewed by entering tensorboard --logdir=runs in Terminal.\n",
    "\n",
    "- Import from torch.utils.tensorboard import SummaryWriter\n",
    "- Initialize tb = SummaryWriter()\n",
    "- Write using tb.add_scalar, tb.add_histogram, comments, etc.\n",
    "- Close the writer, tb.close()\n",
    "- View the interface from the terminal tensorboard --logdir=runs.\n",
    "\n",
    "**Early-stopping**\n",
    "We can set a prev_loss variable to save the last loss value calculated (best done for validation loss). If the loss increases beyond a set tolerance (the network starts overfitting), we stop the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: tensor(45402) loss: 379.31063798069954\n",
      "epoch 1 total_correct: tensor(50253) loss: 263.45164558291435\n",
      "epoch 2 total_correct: tensor(51114) loss: 238.57917414605618\n",
      "epoch 3 total_correct: tensor(51582) loss: 227.86232540011406\n",
      "epoch 4 total_correct: tensor(51766) loss: 221.73263746500015\n",
      "epoch 5 total_correct: tensor(51872) loss: 219.1599306613207\n",
      "epoch 6 total_correct: tensor(52094) loss: 213.07448703050613\n",
      "epoch 7 total_correct: tensor(52135) loss: 213.0996966511011\n",
      "epoch 8 total_correct: tensor(52286) loss: 208.63510642945766\n",
      "epoch 9 total_correct: tensor(52338) loss: 206.93188540637493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, channels=1): # default grayscale\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=6, kernel_size=5) \n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # ((28-5+1)/2 -5 +1)/2 = 4\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t) # activation_function='relu' in tf.keras      \n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t) # activation_funcion='relu' in tf.keras\n",
    "        \n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t, dim=1) # first index is batch\n",
    "        return t\n",
    "\n",
    "\n",
    "def get_num_correct(preds, labels):\n",
    "    return (preds.argmax(dim=1) == labels).sum()\n",
    "\n",
    "\n",
    "# Get data\n",
    "\n",
    "# Compile network\n",
    "network = Network()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Train for 10 epochs\n",
    "for epoch in range(10): \n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch \n",
    "        preds = network(images)\n",
    "        \n",
    "        loss = F.cross_entropy(preds, labels) # loss function\n",
    "        optimizer.zero_grad()                 # set all gradients to zero\n",
    "        \n",
    "        loss.backward()         # calculate gradients, training points are supply constants\n",
    "        optimizer.step()        # update weights to minimize loss (accdg to adam)\n",
    "\n",
    "        total_loss += loss.item() \n",
    "        total_correct += get_num_correct(preds, labels)\n",
    "        \n",
    "    print(\n",
    "        \"epoch\", epoch, \n",
    "        \"total_correct:\", total_correct, \n",
    "        \"loss:\", total_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds_labels(model, loader): # simple concat\n",
    "    all_preds  = torch.tensor([])\n",
    "    all_labels = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        all_preds  = torch.cat((all_preds,  model(images)),  dim=0)\n",
    "        all_labels = torch.cat((all_labels, labels.float()), dim=0)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "\n",
    "with torch.no_grad(): # disable gradient tracking!\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=60000)\n",
    "    train_preds, train_labels = get_all_preds_labels(network, train_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 10])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds.shape # predictions in the form of probability dists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.argmax(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_true, y_pred):\n",
    "    C = np.zeros((10, 10), dtype=int)\n",
    "    for k in range(len(y_true)):\n",
    "        i = actual[k]   \n",
    "        j = predicted[k]\n",
    "        C[i, j] += 1\n",
    "    return C\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual    = train_labels.int().numpy()\n",
    "predicted = train_preds.argmax(dim=1).int().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "(confusion_matrix(actual, predicted) == \n",
    " sklearn.metrics.confusion_matrix(actual, predicted)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# simple man plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(C)\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(C/60, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, \n",
    "                          normalize=False, \n",
    "                          title='Confusion matrix', \n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    # print the cm\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "    print(cm)\n",
    "    \n",
    "    # plot the cm\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd' # float or int\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), \n",
    "                 horizontalalignment=\"center\", \n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\") # color of text\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "names = (\n",
    "    'T-shirt/top'\n",
    "    ,'Trouser'\n",
    "    ,'Pullover'\n",
    "    ,'Dress'\n",
    "    ,'Coat'\n",
    "    ,'Sandal'\n",
    "    ,'Shirt'\n",
    "    ,'Sneaker'\n",
    "    ,'Bag'\n",
    "    ,'Ankle boot'\n",
    ")\n",
    "plt.figure(figsize=(10,10), dpi=200)\n",
    "plot_confusion_matrix(C, names)\n",
    "if matplotlib.__version__ == '3.1.1':\n",
    "    axes = plt.gca()\n",
    "    a = axes.get_ylim()\n",
    "    plt.ylim(a[1]-0.5, a[0]+0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
